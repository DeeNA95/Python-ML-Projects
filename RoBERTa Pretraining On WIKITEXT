{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1408148,"sourceType":"datasetVersion","datasetId":823358}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T17:28:39.548912Z","iopub.execute_input":"2024-09-29T17:28:39.549734Z","iopub.status.idle":"2024-09-29T17:28:40.649939Z","shell.execute_reply.started":"2024-09-29T17:28:39.549689Z","shell.execute_reply":"2024-09-29T17:28:40.648866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os \nfrom tokenizers import ByteLevelBPETokenizer\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:28:00.331572Z","iopub.execute_input":"2024-09-29T21:28:00.332288Z","iopub.status.idle":"2024-09-29T21:28:00.337010Z","shell.execute_reply.started":"2024-09-29T21:28:00.332246Z","shell.execute_reply":"2024-09-29T21:28:00.336075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom pathlib import Path\nfrom tokenizers import ByteLevelBPETokenizer\nimport os\n\n# Initialize the tokenizer\ntokenizer = ByteLevelBPETokenizer()\n\n# Train the tokenizer on the dataset\ntokenizer.train(\n    \"/kaggle/input/wikitext/wikitext-103-raw/wiki.train.raw\", \n    vocab_size=52_000, \n    min_frequency=2, \n    special_tokens=[\n        \"<s>\",    # Start of sentence\n        \"<pad>\",  # Padding token\n        \"</s>\",   # End of sentence\n        \"<unk>\",  # Unknown token\n        \"<mask>\", # Mask token\n        \"=\",      # Single equal sign\n        \"==\",     # Double equal sign\n        \"'''\",    # Bold\n        \"''\",     # Italics\n        \"[[\",     # Start of a link\n        \"]]\",     # End of a link\n        \"<ref>\",  # Reference tag\n        \"{{\",     # Start of a template\n        \"}}\",     # End of a template\n        \"<!--\",    # Start of a comment\n        \"-->\"     # End of a comment\n    ]\n)\n\n# Save the tokenizer to disk\nmodel_dir = '/kaggle/working/model'\nos.makedirs(model_dir, exist_ok=True)\ntokenizer.save_model(model_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T17:28:45.549628Z","iopub.execute_input":"2024-09-29T17:28:45.550241Z","iopub.status.idle":"2024-09-29T17:30:38.975157Z","shell.execute_reply.started":"2024-09-29T17:28:45.550195Z","shell.execute_reply":"2024-09-29T17:30:38.973543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\n\ntokenizer = ByteLevelBPETokenizer(\n    '/kaggle/working/model/vocab.json',\n    '/kaggle/working/model/merges.txt'\n)\n\ntokenizer._tokenizer.post_processor = BertProcessing(\n    ('<s>', tokenizer.token_to_id('<s>')),  # start of sequence token\n    ('</s>', tokenizer.token_to_id('</s>'))  # end of sequence token\n)\ntokenizer.enable_truncation(max_length=512)\n\nprint(tokenizer.encode('Wikitext encoder training').tokens)\nprint(tokenizer.encode('Wikitext encoder training'))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:57:59.282452Z","iopub.execute_input":"2024-09-29T21:57:59.283352Z","iopub.status.idle":"2024-09-29T21:57:59.549114Z","shell.execute_reply.started":"2024-09-29T21:57:59.283309Z","shell.execute_reply":"2024-09-29T21:57:59.548130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! head /kaggle/working/model/merges.txt","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:59:54.002412Z","iopub.execute_input":"2024-09-29T21:59:54.002821Z","iopub.status.idle":"2024-09-29T21:59:55.116516Z","shell.execute_reply.started":"2024-09-29T21:59:54.002785Z","shell.execute_reply":"2024-09-29T21:59:55.115296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaConfig, RobertaTokenizer\n\nconfig = RobertaConfig(\n    vocab_size=52000,\n    max_position_embeddings = 512,\n    num_attention_heads = 12,\n    num_hidden_layers=6,\n    type_vocab_size=1\n)\n\ntokenizer = RobertaTokenizer.from_pretrained('/kaggle/working/model/', max_length=512, clean_up_tokenization_spaces=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:31:16.460315Z","iopub.execute_input":"2024-09-29T21:31:16.461238Z","iopub.status.idle":"2024-09-29T21:31:16.607138Z","shell.execute_reply.started":"2024-09-29T21:31:16.461194Z","shell.execute_reply":"2024-09-29T21:31:16.605780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaForMaskedLM\n\nmodel = RobertaForMaskedLM(config=config).to('cuda')\nprint(model.num_parameters())","metadata":{"execution":{"iopub.status.busy":"2024-09-29T17:40:26.646194Z","iopub.execute_input":"2024-09-29T17:40:26.646571Z","iopub.status.idle":"2024-09-29T17:40:29.928587Z","shell.execute_reply.started":"2024-09-29T17:40:26.646537Z","shell.execute_reply":"2024-09-29T17:40:29.927639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path='/kaggle/input/wikitext/wikitext-103-raw/wiki.train.raw',\n    block_size=128\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=.2\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:45:53.988746Z","iopub.status.idle":"2024-09-29T21:45:53.989153Z","shell.execute_reply.started":"2024-09-29T21:45:53.988937Z","shell.execute_reply":"2024-09-29T21:45:53.988955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check dataset size\nprint(f\"Dataset size: {len(dataset)}\")\nprint(next(iter(dataset)))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:26:55.420117Z","iopub.execute_input":"2024-09-29T21:26:55.420857Z","iopub.status.idle":"2024-09-29T21:26:55.475377Z","shell.execute_reply.started":"2024-09-29T21:26:55.420816Z","shell.execute_reply":"2024-09-29T21:26:55.474250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom transformers import Trainer, TrainingArguments\n\nt_args = TrainingArguments(\n    output_dir = '/kaggle/working/model',\n    overwrite_output_dir = True,\n    learning_rate=2e-5,\n    num_train_epochs=1, #just one for a short train time\n    per_device_train_batch_size=64,\n    save_steps=10000,\n    save_total_limit=5,\n    report_to=[]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=t_args,\n    data_collator=data_collator,\n    train_dataset=dataset\n)\n\ntrainer.train()\ntrainer.save_model('/kaggle/working/model')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T18:01:18.468717Z","iopub.execute_input":"2024-09-29T18:01:18.469109Z","iopub.status.idle":"2024-09-29T21:24:25.491043Z","shell.execute_reply.started":"2024-09-29T18:01:18.469072Z","shell.execute_reply":"2024-09-29T21:24:25.490055Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nmodel_path = '/kaggle/working/model/checkpoint-18204'  \nmodel = RobertaForMaskedLM.from_pretrained(model_path).to('cuda')\ntokenizer = RobertaTokenizer.from_pretrained('/kaggle/working/model')\n\nfill_mask = pipeline(\n    'fill-mask',\n    model=model,\n    tokenizer=tokenizer,\n    device='cuda'\n)\n\nfill_mask(\"Valkyrie is a <mask> game. Playing it is enjoyable\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:47:22.678895Z","iopub.execute_input":"2024-09-29T21:47:22.679795Z","iopub.status.idle":"2024-09-29T21:47:23.332656Z","shell.execute_reply.started":"2024-09-29T21:47:22.679752Z","shell.execute_reply":"2024-09-29T21:47:23.331645Z"},"trusted":true},"execution_count":null,"outputs":[]}]}